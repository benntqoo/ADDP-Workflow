# ğŸš€ Universal AI Coding Framework éƒ¨ç½²æŒ‡å—

## ğŸ“‹ ç³»ç»Ÿè¦æ±‚

### åŸºç¡€ç¯å¢ƒ
- **Python**: 3.8+
- **æ“ä½œç³»ç»Ÿ**: Windows 10+, macOS 10.15+, Linux Ubuntu 18.04+
- **å†…å­˜**: å»ºè®® 8GB+ (Ollama æ¨¡å‹éœ€è¦)
- **å­˜å‚¨**: è‡³å°‘ 10GB å¯ç”¨ç©ºé—´

### ä¾èµ–æœåŠ¡
- **Ollama**: æœ¬åœ° LLM æœåŠ¡ ([å®‰è£…æŒ‡å—](https://ollama.ai))
- **Git**: ç‰ˆæœ¬æ§åˆ¶ (å¯é€‰ï¼Œç”¨äºå…‹éš†é¡¹ç›®)

## ğŸ—ï¸ å®‰è£…éƒ¨ç½²

### æ–¹æ³•ä¸€ï¼šä¸€é”®å¿«é€Ÿéƒ¨ç½² (æ¨è)

```bash
# 1. å…‹éš†é¡¹ç›®
git clone https://github.com/your-org/universal-ai-coding-framework.git
cd universal-ai-coding-framework

# 2. è¿è¡Œå¿«é€Ÿå¯åŠ¨è„šæœ¬
python scripts/quick_start.py
```

å¿«é€Ÿå¯åŠ¨è„šæœ¬ä¼šè‡ªåŠ¨ï¼š
- âœ… æ£€æŸ¥ç¯å¢ƒä¾èµ–
- âœ… å®‰è£… Python ä¾èµ–
- âœ… æ£€æŸ¥ Ollama æœåŠ¡
- âœ… åˆå§‹åŒ– .addp é¡¹ç›®ç»“æ„
- âœ… ç”Ÿæˆé…ç½®æ–‡ä»¶
- âœ… æ˜¾ç¤ºåç»­é…ç½®æ­¥éª¤

### æ–¹æ³•äºŒï¼šæ‰‹åŠ¨éƒ¨ç½²

#### æ­¥éª¤ 1: å®‰è£… Ollama
```bash
# macOS/Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Windows
# ä¸‹è½½å¹¶å®‰è£…: https://ollama.ai/download
```

#### æ­¥éª¤ 2: å®‰è£… Python ä¾èµ–
```bash
pip install -r requirements.txt
```

#### æ­¥éª¤ 3: ä¸‹è½½æ¨èæ¨¡å‹
```bash
ollama pull qwen2.5:14b
```

#### æ­¥éª¤ 4: åˆå§‹åŒ–é¡¹ç›®ç»“æ„
```bash
python main.py --init
```

#### æ­¥éª¤ 5: ç”Ÿæˆé…ç½®æ–‡ä»¶
```bash
python main.py --save-config
```

## ğŸ”§ AI å·¥å…·é…ç½®

### Claude Code é…ç½®

1. **æ·»åŠ  MCP æœåŠ¡å™¨**
```bash
claude config mcp-servers add universal-coding-assistant
```

2. **é…ç½®æœåŠ¡å™¨è·¯å¾„**
åœ¨ Claude Code é…ç½®ä¸­æ·»åŠ ï¼š
```json
{
  "mcpServers": {
    "universal-coding-assistant": {
      "command": "python",
      "args": ["main.py"],
      "cwd": "/path/to/universal-ai-coding-framework"
    }
  }
}
```

### Gemini CLI é…ç½®

1. **é…ç½® MCP æœåŠ¡å™¨**
```bash
gemini config mcp-servers.universal-coding-assistant.command "python main.py"
gemini config mcp-servers.universal-coding-assistant.cwd "/path/to/framework"
```

2. **éªŒè¯é…ç½®**
```bash
gemini config list
```

### Cursor é…ç½®

1. **åˆ›å»º MCP é…ç½®æ–‡ä»¶**
åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º `.cursor/mcp.json`ï¼š
```json
{
  "mcpServers": {
    "universal-coding-assistant": {
      "command": "python",
      "args": ["main.py"],
      "cwd": "./universal-ai-coding-framework"
    }
  }
}
```

2. **é‡å¯ Cursor** ä»¥åŠ è½½é…ç½®

## ğŸ§ª åŠŸèƒ½æµ‹è¯•

### åŸºç¡€åŠŸèƒ½æµ‹è¯•

#### 1. é¡¹ç›®åˆå§‹åŒ–æµ‹è¯•
```bash
# Claude Code
claude "åˆå§‹åŒ– ADDP é¡¹ç›®ç»“æ„"

# Gemini CLI
gemini "è®¾ç½®ç»Ÿä¸€ç¼–ç¨‹ç¯å¢ƒ"

# é¢„æœŸç»“æœ: åˆ›å»ºå®Œæ•´çš„ .addp/ ç›®å½•ç»“æ„
```

#### 2. æŸ¥è¯¢ä¼˜åŒ–æµ‹è¯•
```bash
# Claude Code
claude "ä¼˜åŒ–è¿™ä¸ªæŸ¥è¯¢: å®ç°ç”¨æˆ·ç™»å½•åŠŸèƒ½"

# é¢„æœŸç»“æœ: è¿”å›è¯¦ç»†çš„æŠ€æœ¯è§„æ ¼å’Œå®æ–½å»ºè®®
```

#### 3. ADDP å·¥ä½œæµæµ‹è¯•
```bash
# Claude Code
claude "å¯åŠ¨ ADDP åˆ†æé˜¶æ®µ"

# é¢„æœŸç»“æœ: ç”Ÿæˆåˆ†æé˜¶æ®µçš„æ¨¡æ¿å’Œæ£€æŸ¥æ¸…å•
```

#### 4. è·¨å·¥å…·åŒæ­¥æµ‹è¯•
```bash
# åœ¨ Claude Code ä¸­
claude "ä¿å­˜å½“å‰é¡¹ç›®çŠ¶æ€"

# åœ¨ Gemini CLI ä¸­
gemini "åŠ è½½é¡¹ç›®çŠ¶æ€"

# é¢„æœŸç»“æœ: çŠ¶æ€ä¿¡æ¯æˆåŠŸåŒæ­¥
```

### é«˜çº§åŠŸèƒ½æµ‹è¯•

#### è§„æ ¼é©±åŠ¨å¼€å‘æµç¨‹
```bash
# 1. åˆ›å»ºéœ€æ±‚è§„æ ¼
claude "/specify å®ç°ç”¨æˆ·æ³¨å†ŒåŠŸèƒ½ï¼ŒåŒ…å«é‚®ç®±éªŒè¯"

# 2. ç”ŸæˆæŠ€æœ¯æ–¹æ¡ˆ
claude "/plan"

# 3. åˆ†è§£å¼€å‘ä»»åŠ¡
claude "/tasks"

# 4. å¯åŠ¨ ADDP å·¥ä½œæµ
claude "/workflow analysis"
```

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. Ollama è¿æ¥å¤±è´¥
**ç—‡çŠ¶**: æŸ¥è¯¢ä¼˜åŒ–åŠŸèƒ½æŠ¥é”™ "Ollama API é”™è¯¯"

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥ Ollama æœåŠ¡çŠ¶æ€
ollama serve

# éªŒè¯æ¨¡å‹å®‰è£…
ollama list

# æµ‹è¯• API è¿æ¥
curl http://localhost:11434/api/generate -d '{"model":"qwen2.5:14b","prompt":"test"}'
```

#### 2. MCP å·¥å…·ä¸å¯ç”¨
**ç—‡çŠ¶**: AI å·¥å…·æ— æ³•è¯†åˆ« MCP å‘½ä»¤

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥é…ç½®æ–‡ä»¶
python main.py --dev

# éªŒè¯ MCP æœåŠ¡å™¨çŠ¶æ€
python -c "from src.mcp_server.server import create_mcp_server; print('MCP OK')"
```

#### 3. .addp ç›®å½•ç»“æ„å¼‚å¸¸
**ç—‡çŠ¶**: ç›®å½•ç»“æ„ä¸å®Œæ•´æˆ–ç¼ºå¤±

**è§£å†³æ–¹æ¡ˆ**:
```bash
# é‡æ–°åˆå§‹åŒ–
python main.py --init

# æ£€æŸ¥æƒé™
ls -la .addp/
```

#### 4. ä¾èµ–å®‰è£…å¤±è´¥
**ç—‡çŠ¶**: pip install æŠ¥é”™

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å‡çº§ pip
python -m pip install --upgrade pip

# ä½¿ç”¨å›½å†…é•œåƒ
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple/
```

### æ—¥å¿—è°ƒè¯•

#### å¯ç”¨è¯¦ç»†æ—¥å¿—
```bash
# å¼€å‘æ¨¡å¼ (è¯¦ç»†æ—¥å¿—)
python main.py --dev

# è®¾ç½®æ—¥å¿—çº§åˆ«
export PYTHONPATH=. LOGLEVEL=DEBUG python main.py
```

#### æ£€æŸ¥æ—¥å¿—æ–‡ä»¶
```bash
# æŸ¥çœ‹ MCP æœåŠ¡å™¨æ—¥å¿—
tail -f .addp/analytics/logs/mcp_server.log

# æŸ¥çœ‹æŸ¥è¯¢ä¼˜åŒ–æ—¥å¿—
tail -f .addp/analytics/logs/query_optimization.log
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### Ollama ä¼˜åŒ–
```bash
# è®¾ç½®ç¯å¢ƒå˜é‡ä¼˜åŒ–æ€§èƒ½
export OLLAMA_HOST=0.0.0.0:11434
export OLLAMA_MODELS=/path/to/models
export OLLAMA_NUM_PARALLEL=4
```

### ç¼“å­˜é…ç½®
ç¼–è¾‘ `.addp/configs/mcp/server_config.json`:
```json
{
  "project": {
    "cache_enabled": true,
    "cache_ttl": 86400,
    "max_cache_size": "500MB"
  }
}
```

### å†…å­˜ä¼˜åŒ–
```bash
# å¯¹äº 8GB å†…å­˜ç³»ç»Ÿï¼Œä½¿ç”¨è¾ƒå°æ¨¡å‹
ollama pull qwen2.5:7b

# é…ç½®æ–‡ä»¶ä¸­ä¿®æ”¹æ¨¡å‹
{
  "ollama": {
    "model": "qwen2.5:7b"
  }
}
```

## ğŸ”„ æ›´æ–°å‡çº§

### æ›´æ–°æ¡†æ¶
```bash
# æ‹‰å–æœ€æ–°ä»£ç 
git pull origin main

# æ›´æ–°ä¾èµ–
pip install -r requirements.txt --upgrade

# é‡æ–°åˆå§‹åŒ– (ä¿ç•™ç°æœ‰æ•°æ®)
python main.py --init
```

### å¤‡ä»½æ•°æ®
```bash
# å¤‡ä»½é¡¹ç›®è®°å¿†å’Œé…ç½®
tar -czf addp_backup_$(date +%Y%m%d).tar.gz .addp/
```

## ğŸš€ ç”Ÿäº§éƒ¨ç½²

### Docker éƒ¨ç½² (æ¨è)
```dockerfile
# åˆ›å»º Dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY . .
RUN pip install -r requirements.txt

EXPOSE 8000
CMD ["python", "main.py"]
```

### ç³»ç»ŸæœåŠ¡
```bash
# åˆ›å»º systemd æœåŠ¡
sudo tee /etc/systemd/system/universal-coding.service << EOF
[Unit]
Description=Universal AI Coding Framework
After=network.target

[Service]
Type=simple
User=your-user
WorkingDirectory=/path/to/framework
ExecStart=/usr/bin/python main.py
Restart=always

[Install]
WantedBy=multi-user.target
EOF

sudo systemctl enable universal-coding
sudo systemctl start universal-coding
```

## ğŸ“ æ”¯æŒä¸åé¦ˆ

- **GitHub Issues**: [æäº¤é—®é¢˜](https://github.com/your-org/universal-ai-coding-framework/issues)
- **æ–‡æ¡£**: [åœ¨çº¿æ–‡æ¡£](https://docs.universal-ai-coding.org)
- **ç¤¾åŒº**: [GitHub Discussions](https://github.com/your-org/universal-ai-coding-framework/discussions)

---

ğŸ‰ **éƒ¨ç½²å®Œæˆï¼å¼€å§‹äº«å—ç»Ÿä¸€çš„ AI ç¼–ç¨‹åä½œä½“éªŒï¼**